{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "kBaTEgK9UDpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# __all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "__all__ = ['ResNet', 'resnet56']\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__  # m for module; \n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "                # zero padding;  (0, 0, 0, 0, planes//4, planes//4) for padding (0,0) for 3 dim, (0,0) for 2 dim, (planes//4, planes//4) for channel(1 dim)\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        # for MNIST\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "5sGUgE3fUHQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(num_classes, in_channels=1):\n",
        "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
        "    def resnet_block(in_channels, out_channels, num_residuals,\n",
        "                     first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(Residual(in_channels, out_channels,\n",
        "                                        use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(out_channels, out_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "    # smaller conv kernel, stride, padding, delete max pooling layer\n",
        "    net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU())\n",
        "    net.add_module(\"resnet_block1\", resnet_block(\n",
        "        64, 64, 2, first_block=True))\n",
        "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
        "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
        "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
        "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
        "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
        "                                       nn.Linear(512, num_classes)))\n",
        "    return net\n",
        "\n",
        "def resnet18_mnist():\n",
        "    b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
        "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
        "                    nn.AdaptiveAvgPool2d((1,1)),\n",
        "                    nn.Flatten(), nn.Linear(512, 10))\n",
        "    return net\n",
        "\n",
        "def resnet56_mnist():\n",
        "    b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
        "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
        "                    nn.AdaptiveAvgPool2d((1,1)),\n",
        "                    nn.Flatten(), nn.Linear(512, 10))\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56(num_classes):\n",
        "    return ResNet(BasicBlock, [9, 9, 9], num_classes)\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])"
      ],
      "metadata": {
        "id": "-6TWtVqOXp7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "-ajuU6CrTHCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_block(input_channels, num_channels, num_residuals,\n",
        "                 first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block: #b2 first block\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk"
      ],
      "metadata": {
        "id": "nx5-0HT2Tb0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()\n",
        "        self.fc1 = nn.Linear(784,512)\n",
        "        self.fc2 = nn.Linear(512,128)\n",
        "        self.fc3 = nn.Linear(128,10)\n",
        "        \n",
        "    def forward(self,din):\n",
        "        din = din.view(-1,28*28)\n",
        "        dout = nn.functional.relu(self.fc1(din))\n",
        "        dout = nn.functional.relu(self.fc2(dout))\n",
        "        return nn.functional.softmax(self.fc3(dout))"
      ],
      "metadata": {
        "id": "WevQqGTL81od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPNet(nn.Module):\n",
        "    def __init__(self, n_hidden_nodes, n_hidden_layers, activation, keep_rate=0):\n",
        "        super(Net, self).__init__()\n",
        "        self.n_hidden_nodes = n_hidden_nodes\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.activation = activation\n",
        "        if not keep_rate:\n",
        "            keep_rate = 0.5\n",
        "        self.keep_rate = keep_rate\n",
        "        # Set up perceptron layers and add dropout\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3,\n",
        "                                   n_hidden_nodes)\n",
        "        self.fc1_drop = nn.Dropout(1 - keep_rate)\n",
        "        if n_hidden_layers == 2:\n",
        "            self.fc2 = nn.Linear(n_hidden_nodes,\n",
        "                                       n_hidden_nodes)\n",
        "            self.fc2_drop = nn.Dropout(1 - keep_rate)\n",
        "\n",
        "        self.out = nn.Linear(n_hidden_nodes, 100) # cifar100\n",
        "        # self.out = nn.Linear(n_hidden_nodes, 10) # cifar10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        if self.activation == \"sigmoid\":\n",
        "            sigmoid = nn.Sigmoid()\n",
        "            x = sigmoid(self.fc1(x))\n",
        "        elif self.activation == \"relu\":\n",
        "            x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        if self.n_hidden_layers == 2:\n",
        "            if self.activation == \"sigmoid\":\n",
        "                x = sigmoid(self.fc2(x))\n",
        "            elif self.activation == \"relu\":\n",
        "                x = nn.functional.relu(self.fc2(x))\n",
        "            x = self.fc2_drop(x)\n",
        "        return nn.functional.log_softmax(self.out(x))"
      ],
      "metadata": {
        "id": "BEakpM8iWbU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}